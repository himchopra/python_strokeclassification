# -*- coding: utf-8 -*-
"""Stroke_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1coPJFDvmyATpXxzsM55EsJ-6vkIloBZk
"""

import pandas as pd
from datetime import datetime, timedelta
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


import imblearn
from imblearn.over_sampling import SMOTE

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import mutual_info_classif

from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
#from sklearn.model_selection import GridSearchCV


from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve
from sklearn.metrics import classification_report
#from sklearn.metrics import accuracy_score
#from sklearn.metrics import precision_recall_curve

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier

df_strokedata = pd.read_csv('stroke_data.csv')
df_strokedata.head()

#Checking Data Types
df_strokedata.info()
#Observation : BMI has null values, 201 null values. Total 11 columns (id is considered as reduntant). Out of 11 columns, stroke is dependent (predictor) colun and remaining 10 as explanatory (independent) variables. Out of these 10 variables 5 looks as categorical and 5 as integer

# Checking nature of each column
df_strokedata.describe(include='all')

#####
#Observation :
#1. Following data looks continous - age, avg_glucose_level, bmi
#2. Following data looks binary - hypertension, heart_disease, stroke
#3. Following columns are categorical - Gender, ever_married, work_type, Residence_type, smoking_status
######

#Starting with categorical variables
columns = ['gender','ever_married','Residence_type','work_type','smoking_status']

for col in columns:
  print(df_strokedata.groupby(by=col)['id'].count())
  print('------')

# Columns with binary values

columns = ['hypertension', 'ever_married', 'heart_disease', 'stroke']

for col in columns:
  print(df_strokedata.groupby(by=col)['id'].count())
  print('------')

# Observation : Disease prevalence rate (Stroke) is 4.87%

# Distribution graph of continous variables

fig, axes = plt.subplots(1, 3, figsize=(12,5), dpi= 80)
sns.distplot(df_strokedata.age,ax=axes[0])
sns.distplot(df_strokedata.avg_glucose_level,ax=axes[1])
sns.distplot(df_strokedata.bmi,ax=axes[2])

def univariate_column(i_col):
  fig, axes = plt.subplots(1, 2, figsize=(12,3), dpi= 80)

  df_groupby = df_strokedata.groupby([i_col, 'stroke'])['id'].count()
  # Get the count of records by gender
  df_total = df_strokedata.groupby([i_col])['id'].count()
  # Get the percentage for 100% stacked bar chart
  df_pct = df_groupby / df_total * 100
  # Create proper DataFrame's format
  df_pct = df_pct.unstack()
  sns.countplot(df_strokedata[i_col], hue = df_strokedata['stroke'],ax=axes[0])
  sns.scatterplot(data=df_pct.iloc[:,1],ax=axes[1])

# Univariate analysis to identify stoke data is dependent on which all variables

# Starting with categorical variables

columns = ['gender','ever_married','Residence_type','work_type']

for col in columns:
  univariate_column(col)

columns = ['hypertension', 'ever_married', 'heart_disease', 'stroke']

for col in columns:
  univariate_column(col)

def univariate_continous(i_col):
  fig, axes = plt.subplots(1, 2, figsize=(12,3), dpi= 80)

  df_groupby = df_strokedata.groupby([i_col, 'stroke'])['id'].count()
  # Get the count of records by gender
  df_total = df_strokedata.groupby([i_col])['id'].count()
  # Get the percentage for 100% stacked bar chart
  df_pct = df_groupby / df_total * 100
  # Create proper DataFrame's format
  df_pct = df_pct.unstack()
  sns.boxplot(data=df_strokedata,x='stroke',y=i_col,ax=axes[0])
  sns.scatterplot(data=df_pct.iloc[:,1],ax=axes[1])

columns = ['age', 'bmi', 'avg_glucose_level']#, 'stroke']

for col in columns:
  univariate_continous(col)

# Creating another copy of input data to preserve original data
df = df_strokedata.copy()
df_strokedata = df_strokedata[df_strokedata.gender != 'Other']

## Encoding Variables

## Where there are only 2 unique values we change values with a flag else we create encoding using dummy
columns = ['gender','ever_married','work_type','Residence_type','smoking_status']

for col in columns:
  print(col)
  print(df_strokedata[col].nunique())

df_strokedata["Residence_type"] = df_strokedata["Residence_type"].apply(lambda x: 1 if x=="Urban" else 0)
df_strokedata["ever_married"] = df_strokedata["ever_married"].apply(lambda x: 1 if x=="Yes" else 0)
df_strokedata["gender"] = df_strokedata["gender"].apply(lambda x: 1 if x=="Male" else 0)


df_strokedata = pd.get_dummies(data=df_strokedata, columns=['smoking_status'])
df_strokedata = pd.get_dummies(data=df_strokedata, columns=['work_type'])
#df_strokedata = pd.get_dummies(data=df_strokedata, columns=['gender'])

# Treating null in BMI data
# Since BMI data is pretty much normally distributed, null values of BMI are replaced with avg

mean_value = df_strokedata['bmi'].mean()
df_strokedata.fillna(mean_value, inplace = True)

df_strokedata.drop(columns=['smoking_status_Unknown','work_type_children'],inplace=True)

# plotting the heat map of correlation matrix using seaborn library
plt.figure(figsize=(25,10),dpi=150)
sns.heatmap(df_strokedata.corr(),annot=True, xticklabels= df_strokedata.corr().columns, yticklabels= df_strokedata.corr().columns)



# Feature Engineering

#SMOTE is an example of over sampling in which we increase the minority samples of the target variable to the majority samples.
oversample = SMOTE()

# generate dataset
df_strokedata_X = df_strokedata.drop(columns = ['id','stroke'])
df_strokedata_y = df_strokedata['stroke']

x_train, x_test, y_train, y_test = train_test_split(df_strokedata_X, df_strokedata_y,test_size=0.2)#, random_state=42)

X_train_smote, y_train_smote = oversample.fit_resample(x_train,y_train)
X_test_smote, y_test_smote = oversample.fit_resample(x_test,y_test)



# ANOVA feature selection for numeric input and categorical output
# define feature selection
fs = SelectKBest(score_func=f_classif, k='all')
# apply feature selection
fs_scores = fs.fit_transform(X_train_smote, y_train_smote)
for i in range(len(fs.scores_)):
 print('Feature %d : %s: %f' % (i, X_train_smote.columns[i], fs.scores_[i]))
# plot the scores
plt.bar([i for i in range(len(fs.scores_))], fs.scores_)
plt.show()

x_train_fs = X_train_smote[['age','ever_married','gender','Residence_type','avg_glucose_level','smoking_status_formerly smoked','smoking_status_never smoked','smoking_status_smokes']]
x_test_fs = X_test_smote[['age','ever_married','gender','Residence_type','avg_glucose_level','smoking_status_formerly smoked','smoking_status_never smoked','smoking_status_smokes']]

# RandomForest
clf = RandomForestClassifier()
clf.fit(x_train_fs, y_train_smote)
y_pred = clf.predict(x_test_fs)

cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)
print("Cross Validation Score : ",'{0:.2%}'.format(cross_val_score(clf,x_train_fs,y_train_smote,cv = cv,scoring = 'roc_auc').mean()))
print("ROC_AUC Score : ",'{0:.2%}'.format(roc_auc_score(y_test_smote,y_pred)))
plot_roc_curve(clf, x_test_fs,y_test_smote)
plt.title('ROC_AUC_Plot')
plt.show()

## Model Evaluation

# Confusion Matrix
cm = confusion_matrix(y_test_smote,clf.predict(x_test_fs))
names = ['True Neg','False Pos','False Neg','True Pos']
counts = [value for value in cm.flatten()]
percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cm,annot = labels,fmt ='')

# Classification Report
print(classification_report(y_test_smote,clf.predict(x_test_fs)))

# Trees

clf = ExtraTreesClassifier()
clf.fit(x_train_fs, y_train_smote)
y_pred = clf.predict(x_test_fs)

cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)
print("Cross Validation Score : ",'{0:.2%}'.format(cross_val_score(clf,x_train_fs,y_train_smote,cv = cv,scoring = 'roc_auc').mean()))
print("ROC_AUC Score : ",'{0:.2%}'.format(roc_auc_score(y_test_smote,y_pred)))
plot_roc_curve(clf, x_test_fs,y_test_smote)
plt.title('ROC_AUC_Plot')
plt.show()

## Model Evaluation

# Confusion Matrix
cm = confusion_matrix(y_test_smote,clf.predict(x_test_fs))
names = ['True Neg','False Pos','False Neg','True Pos']
counts = [value for value in cm.flatten()]
percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cm,annot = labels,fmt ='')

# Classification Report
print(classification_report(y_test_smote,clf.predict(x_test_fs)))

# DecisionTree

clf = DecisionTreeClassifier()
clf.fit(x_train_fs, y_train_smote)
y_pred = clf.predict(x_test_fs)

cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)
print("Cross Validation Score : ",'{0:.2%}'.format(cross_val_score(clf,x_train_fs,y_train_smote,cv = cv,scoring = 'roc_auc').mean()))
print("ROC_AUC Score : ",'{0:.2%}'.format(roc_auc_score(y_test_smote,y_pred)))
plot_roc_curve(clf, x_test_fs,y_test_smote)
plt.title('ROC_AUC_Plot')
plt.show()

## Model Evaluation

# Confusion Matrix
cm = confusion_matrix(y_test_smote,clf.predict(x_test_fs))
names = ['True Neg','False Pos','False Neg','True Pos']
counts = [value for value in cm.flatten()]
percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cm,annot = labels,fmt ='')

# Classification Report
print(classification_report(y_test_smote,clf.predict(x_test_fs)))

# Support Vector
clf = SVC()
clf.fit(x_train_fs, y_train_smote)
y_pred = clf.predict(x_test_fs)

cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)
print("Cross Validation Score : ",'{0:.2%}'.format(cross_val_score(clf,x_train_fs,y_train_smote,cv = cv,scoring = 'roc_auc').mean()))
print("ROC_AUC Score : ",'{0:.2%}'.format(roc_auc_score(y_test_smote,y_pred)))
plot_roc_curve(clf, x_test_fs,y_test_smote)
plt.title('ROC_AUC_Plot')
plt.show()

## Model Evaluation

# Confusion Matrix
cm = confusion_matrix(y_test_smote,clf.predict(x_test_fs))
names = ['True Neg','False Pos','False Neg','True Pos']
counts = [value for value in cm.flatten()]
percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cm,annot = labels,fmt ='')

# Classification Report
print(classification_report(y_test_smote,clf.predict(x_test_fs)))

# XG Boost

clf = XGBClassifier()
clf.fit(x_train_fs, y_train_smote)
y_pred = clf.predict(x_test_fs)

cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)
print("Cross Validation Score : ",'{0:.2%}'.format(cross_val_score(clf,x_train_fs,y_train_smote,cv = cv,scoring = 'roc_auc').mean()))
print("ROC_AUC Score : ",'{0:.2%}'.format(roc_auc_score(y_test_smote,y_pred)))
plot_roc_curve(clf, x_test_fs,y_test_smote)
plt.title('ROC_AUC_Plot')
plt.show()

## Model Evaluation

# Confusion Matrix
cm = confusion_matrix(y_test_smote,clf.predict(x_test_fs))
names = ['True Neg','False Pos','False Neg','True Pos']
counts = [value for value in cm.flatten()]
percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cm,annot = labels,fmt ='')

# Classification Report
print(classification_report(y_test_smote,clf.predict(x_test_fs)))

# Logistics Regression

clf = LogisticRegression()
clf.fit(x_train_fs, y_train_smote)
y_pred = clf.predict(x_test_fs)

cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)
print("Cross Validation Score : ",'{0:.2%}'.format(cross_val_score(clf,x_train_fs,y_train_smote,cv = cv,scoring = 'roc_auc').mean()))
print("ROC_AUC Score : ",'{0:.2%}'.format(roc_auc_score(y_test_smote,y_pred)))
plot_roc_curve(clf, x_test_fs,y_test_smote)
plt.title('ROC_AUC_Plot')
plt.show()

## Model Evaluation

# Confusion Matrix
cm = confusion_matrix(y_test_smote,clf.predict(x_test_fs))
names = ['True Neg','False Pos','False Neg','True Pos']
counts = [value for value in cm.flatten()]
percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cm,annot = labels,fmt ='')

# Classification Report
print(classification_report(y_test_smote,clf.predict(x_test_fs)))

# Logistics Regression

clf = LogisticRegression()
pipe = make_pipeline(StandardScaler(), clf)

pipe.fit(x_train_fs, y_train_smote)
y_pred = pipe.predict(x_test_fs)

cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)
print("Cross Validation Score : ",'{0:.2%}'.format(cross_val_score(pipe,x_train_fs,y_train_smote,cv = cv,scoring = 'roc_auc').mean()))
print("ROC_AUC Score : ",'{0:.2%}'.format(roc_auc_score(y_test_smote,y_pred)))
plot_roc_curve(pipe, x_test_fs,y_test_smote)
plt.title('ROC_AUC_Plot')
plt.show()

## Model Evaluation

# Confusion Matrix
cm = confusion_matrix(y_test_smote,pipe.predict(x_test_fs))
names = ['True Neg','False Pos','False Neg','True Pos']
counts = [value for value in cm.flatten()]
percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cm,annot = labels,fmt ='')

# Classification Report
print(classification_report(y_test_smote,pipe.predict(x_test_fs)))

